# 15. 스쿱(Sqoop)
### 개요
- `SQl to hadoop` 의 줄임말
- 이름 그대로 RDBMS 와 Hadoop 사이의 데이터 이동을 위한 도구
- 현재는 apache attic 으로 옮겨짐 [참고](https://en.wikipedia.org/wiki/Sqoop)
### apache attic
- Apache Software Foundation 에서 더 이상 유지 관리되지 않는 프로젝트들을 보관하는 장소
- 현재 homebrew 에서도 지원 중단 상태

### 스쿱1, 스쿱2
- 스쿱1
  - 명령 줄 도구, 명령 실행시 맵리듀스 작업을 직접 실행
  - 암호 파일 사용
- 스쿱2
  - 서버-클라이언트 아키텍처, REST API 를 통해 서버와 통신하며 서버에서 동작
  - 보안 인증 강화, Kerberos 사용 가능
- 등등의 스쿱2의 장점이 많지만, 책 기준 스쿱1이 널리 활용되고 있다고 함.

### 스쿱 커넥터
- 스쿱이 임포트, 익스포트 하게 해주는 모듈식 컴포넌트
- 다양한 RDMBS 커넥터 제공
- 제네릭 JDBC 커넥터 제공
- 서드파티 커넥터 사용시 일부 Nosql 도 동작 가능

### 스쿱이 사용되었던 이유
- 간결함: 간단한 명령어로 실행
- 하둡 생태계와의 통합성
- 병렬 데이터 전송: 맵리듀스 작업을 통해 병렬로 전송
- 다양한 데이터 타입 제공
- 증분 임포트를 통한 효율적이고 빠른 데이터 전송

### 스쿱의 한계
- RDBMS 를 연결하는데 JDBC 커넥터가 필요: 추가적인 연결이 필요하다는 의미
- Sqoop 의 명령은 atomic 함 -> stop, restart 불가
- RDBMS 에 직접 접근하므로 DB에 부하를 준다

### 스쿱의 대안 Spark
- Spark 는 Rdbms <-> HDFS 뿐 아니라 다양한 데이터소스 연결을 지원
- ETL, 배치 처리, 실시간 스트리밍 처리, 머신러닝, 그래프 처리 등 다양한 작업도 지원
- 클러스터 환경에서 확장에 용이함
- Java 뿐 아니라 Scala, Python, R 언어 등 다양한 언어 지원
- 너무나도 다양한 기능 덕에 학습 곡선이 높다는 단점이 있지만, Spark 로도 Sqoop 의 기능을 모두 지원할 수 있다고 함
