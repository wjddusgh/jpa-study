# 09. 웹 크롤러 설계
웹 크롤러
- 로봇, 스파이더 라고도 부름
- 검색엔진에서 널리 쓰이는 기술
- 웹에 새로 올라오거나 갱신된 컨텐츠(웹페이지, 이미지, 비디오, pdf 등)를 찾아내는 목적
- 이용 예시
  - 검색 엔진 인덱싱
  - 웹 아카이빙
  - 웹 마이닝
  - 웹 모니터링

## 1단계) 문제 이해 및 설계 범위 과정
웹 크롤러의 기본 알고리즘
1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹페이지 다운로드
2. 다운받은 웹페이지에서 URL들 추출
3. 추출된 URL들을 다운로드할 URL목록에 추가하고, 위의 과정을 처음부터 반복

실제로는 웹 크롤러는 규모 확장성을 위해 더 복잡하게 설계되어 있음

### 문제 요구사항
- 웹 크롤러 용도 : 검색 엔진 인덱싱
- 1달에 10억개의 웹 페이지 수집
- 수집한 웹페이지 5년간 저장
- 중복 컨텐츠 무시
### 좋은 웹 크롤러가 만족시켜야할 속성
- 규모 확장성 : 병행성을 잘 활용하자
- 안정성(robustness) : 잘못 작성된 html, 반응없는 서버, 장애, 악성 링크 등의 함정에 잘 대응할 수 있어야 한다
- 예절(politeness) : 수집 대상 웹 사이트에 짧은 시간동안 너무 많은 요청을 보내선 안된다
- 확장성(extensibility) : 새로운 형태의 컨텐츠를 지원하기가 쉬워야 한다

### 개략적 규모 추정
- QPS : 400 페이지/초
- 최대 QPS : 800
- 웹페이지 크기 평균 : 500K
- 필요한 저장 용량 : 500PB

## 2단계) 개략저
- 
