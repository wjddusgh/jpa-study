# 09. 웹 크롤러 설계
웹 크롤러
- 로봇, 스파이더 라고도 부름
- 검색엔진에서 널리 쓰이는 기술
- 웹에 새로 올라오거나 갱신된 컨텐츠(웹페이지, 이미지, 비디오, pdf 등)를 찾아내는 목적
- 이용 예시
  - 검색 엔진 인덱싱
  - 웹 아카이빙
  - 웹 마이닝
  - 웹 모니터링

## 1단계) 문제 이해 및 설계 범위 과정
웹 크롤러의 기본 알고리즘
1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹페이지 다운로드
2. 다운받은 웹페이지에서 URL들 추출
3. 추출된 URL들을 다운로드할 URL목록에 추가하고, 위의 과정을 처음부터 반복

실제로는 웹 크롤러는 규모 확장성을 위해 더 복잡하게 설계되어 있음

### 문제 요구사항
- 웹 크롤러 용도 : 검색 엔진 인덱싱
- 1달에 10억개의 웹 페이지 수집
- 수집한 웹페이지 5년간 저장
- 중복 컨텐츠 무시
### 좋은 웹 크롤러가 만족시켜야할 속성
- 규모 확장성 : 병행성을 잘 활용하자
- 안정성(robustness) : 잘못 작성된 html, 반응없는 서버, 장애, 악성 링크 등의 함정에 잘 대응할 수 있어야 한다
- 예절(politeness) : 수집 대상 웹 사이트에 짧은 시간동안 너무 많은 요청을 보내선 안된다
- 확장성(extensibility) : 새로운 형태의 컨텐츠를 지원하기가 쉬워야 한다

### 개략적 규모 추정
- QPS : 400 페이지/초
- 최대 QPS : 800
- 웹페이지 크기 평균 : 500K
- 필요한 저장 용량 : 500PB

## 2단계) 개략적 설계안 제시 및 동의 구하기
![image (2)](https://github.com/wjddusgh/jpa-study/assets/69251780/9c4b6035-70fb-4b47-abed-711687acb95e)

### 시작 URL 집합
- 웹 크롤러가 크롤링 시작하는 출발점
- 다양한 전략 사용 가능
  - 공식 포털 사이트나 주요 뉴스 사이트 : 매일 다양한 컨텐츠가 업데이트 되며, 다수의 외부링크 포함
  - 인기있는 SNS : 최신 트렌드 정보가 빠른 업데이트
  - 분야별 전문 사이트

### 미수집 URL 저장소
다운로드할 URL 저장, 관리하는 컴포넌트. FIFO 큐 형태

### HTML 다운로더
인터넷에서 웹 페이지를 다운로드하는 컴포넌트, 다운로드할 URL 은 미수집 URL 저장소가 제공

### 도메인 이름 변환기
URL 을 IP 주소로 변환

### 컨텐츠 파서
- 다운로드한 웹페이지 파싱, 검증 절차
- 크롤링 서버 안에 컨텐츠 파서 구현시 크롤링 과정이 느려질 수 있으므로 독립된 컴포넌트로 만듦

### 중복 컨텐츠?
- 웹 페이지의 29% 가량은 중복 컨텐츠임
- 중복 여부는 웹 페이지 문자열 비교도 있지만, 효과적인 방법은 웹페이지의 해시 값 비교

### 컨텐츠 저장소
- HTML 문서를 보관하는 시스템
- 데이터 양이 많으므로 대부분의 컨텐츠는 디스크에 저장
- 인기 컨텐츠는 메모리에 두어 접근 지연시간 줄임

### URL 추출기
- HTML 페이지 파싱하여 링크들을 골라냄
- 상대경로는 전부 절대경로로 변환
### URL 필터
특정 컨텐츠 타입이나 파일 확장자 갖는 URL, 접속시 오류 발생하는 URL, 접근 제외 목록에 포함된 URL 걸러내는 역할

### 이미 방문한 URL?
블룸 필터나 해시 테이블 사용해서 같은 URL 중복 처리 방지

### URL 저장소
이미 방문한 URL 보관하는 저장소

## 3단계) 상세 설계
### DFS, BFS
- 웹은 유향 그래프와 같다(페이지: 노드, 하이퍼링크: 엣지)
- DFS 는 좋은 선택이 아니다
  - 그래프 크기가 클 경우 어느정도로 깊이 가게 될지 가늠하기 어려움
  - 특정 주제의 심층 탐색 정도에만 쓰이는 듯
- 주로 BFS 사용하지만 문제점도 있음
  - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아가는데, 그대로 크롤링 병렬로 진행하면 부하가 커져 **예의없는 크롤러**로 간주됨
  - URL 간 **우선순위**를 두지 못하는 아쉬움

## 미수집 URL 저장소     
- BFS 의 문제점을 해결 가능한 요소
### 예의
- 동일 웹사이트에 대해서는 한번에 한 페이지만 요청하는 원칙을 만들자
- 같은 호스트에 속한 URL 은 언제나 같은 큐로 가도록 하고, 한 큐에서 여러개 꺼내서 처리하지 않도록 하자
- 큐 라우터 : 같은 호스트에 속한 URL 을 같은 큐로 가도록 보장하는 역할
- 매핑 테이블 : 호스트 이름과 큐 사이의 관계를 보관하는 테이블
- FIFO 큐 : 큐 라우터를 통해 나뉜 URL 이 저장될 큐, 같은 큐엔 같은 호스트에 속한 URL 만 있다
- 큐 선택기 : 큐들을 순회하면서 URL 을 꺼내서 지정된 작업 스레드에 전달
- 작업 스레드 : 전달된 URL을 다운로드하는 작업 수행, 작업은 순차적으로 처리될 것이며 작업들 사이에는 일정한 지연시간을 둘 수 있다

### 우선순위
- 유용성에 따라 URL 우선순위를 나눌 때는 페이지랭크, 트래픽 양, 갱신 빈도 등 다양한 척도 사용 가능함
- 순위 결정 장치 : URL 입력 받아 우선순위 계산
- 큐 : 우선순위 별로 큐가 하나씩 할당됨, 우선순위 높으면 선택 확률도 올라감
- 큐 선택기 : 임의 큐에서 처리할 URL 꺼내는 역할 담당, 순위가 높은 큐에서 더 자주 꺼내도록 프로그램 됨

![image (3)](https://github.com/wjddusgh/jpa-study/assets/69251780/14f35613-6efa-46d0-a2cd-d11f1df4fbfd)

### 신선도
- 웹 페이지 재수집 여부 판단
  - 웹페이지 변경 이력 활용
  - 우선순위 활용하여, 중요한 페이지는 좀 더 자주 재수집

## HTML 다운로더

### Robots.txt(로봇 제외 프로토콜)
```
User-agent: *
Disallow: /
Allow : /$ 
```
- 웹 사이트가 크롤러와 소통하는 표준적 방법
- 크롤러가 수집해도 되는 페이지 목록이 들어있으므로, 크롤러는 이 규칙을 먼저 확인해야함
- 크롤러는 주기적으로 Robots.txt 다운받아 캐시에 보관할 것

### 성능 최적화
- 분산 크롤링
  - 크롤링 작업을 여러 서버에 분산하는 방법, 각 서버는 여러 스레들르 돌려 다운로드 작업 처리
  - 도메인 이름 변환 결과 캐시
  - 지역성 : 크롤링 작업을 수행하는 서버를 지역별로 분산
  - 짧은 타임아웃 설정
### 안정성
- 안정 해시 : 다운로더 서버를 쉽게 추가,삭제 할 수 있어짐
- 크롤링 상태 및 수집 데이터 저장
- 예외 처리
- 데이터 검증

### 확장성
### 문제있는 컨텐츠 감지 및 회피
