# 9. 웹 로봇
- 웹 로봇: 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 종류 : 크롤러, 스파이더, 웜, 봇 등

## 9.1 크롤러와 크롤링
- 웹 크롤러(스파이더) : 페이지를 가져오고, 그 페이지가 가리키는 모든 페이지를 또 가져오는 재귀 순회
### 어디에서 시작하는가: '루트 집합'
- 루트 집합 : 크롤러가 방문을 시작하는 url 의 초기 집합
- 웹에서 완탐은 없다
- 고립된 문서들도 존재
- 일반적으로 좋은 루트집합은 크고 인기있는 웹 사이트(네이버), 새로 생성된 페이지 목록, 자주 링크안되는 페이지 목록으로 구성됨
- 사용자들이 루트 집합에 새 페이지를 추가하는 기능 제공
### 링크 추출과 상대 링크 정상화
- 꾸준히 탐색, 파싱하면서 새로운 url 링크 들을 절대 url 로 변환
### 순환 피하기
- 순환을 피하기 위해 방문 여부 확인
### 루프와 중복
- 순환은 루프에 빠지게 만듦
- 루프가 아니더라도 중복된 문서는 서버의 부담이 되고, 대역폭 낭비, 중복된 컨텐츠를 만듦
### 빵 부스러기의 흔적
- 수억 개 이상의 url 을 방문 여부 파악하는 자료구조는 빠른 검색이 가능해야함
- 트리와 해시 테이블
- 느슨한 존재 비트맵
- 체크포인트 : 로봇의 갑작스러운 중단을 대비해, 방문한 url 목록이 디스크에 저장됐는지 확인
- 파티셔닝 : 여러 컴퓨터에서 같이 크롤링을 수행
### 별칭과 로봇 순환
- url은 별칭을 가질수 있어서 방문여부 확인이 쉽지 않을 때도 있다
  - 기본 포트
  - url 인코딩(~ -> %7F)
  - 태그여부
  - 대소문자 구분여부
  - 기본페이지가 index.html 이라 생략 가능한 경우
  - ip 주소와 dns
### URL 정규화하기
- 같은 리소스를 가진 url 을 유일하게끔 만들기 위해 정규화 한다
  - 포트없을때 기본포트, 태그 제거, 이스케이핑문자 변환 등
  - 하지만 이걸로도 한계는 있음(위에서 1,2,3만 해결가능)
### 파일 시스템 링크 순환
- 파일 시스템의 심볼릭 링크는 무한한 디렉토리 계층 마들 수 있어서 순환 유발 가능
- 이를 이용해 웹 마스터가 로봇 함정으로 만들어놓기 가능
### 동적 가상 웹 공간
- cgi, api 등이 웹에서 무한한 url 생성 가능하다
  - 달력, 나쁜 심보가진 애들 등
- 로봇이 cgi, api(얜 어캐알지? 쿼리스트링으로?) 포함하면 크롤링 거부
- 요즘은 챗지피티 문의해봄
  - 자바스크립트 렌더링 지원: 많은 현대 웹 크롤러들은 자바스크립트를 렌더링할 수 있습니다. 이를 통해 동적으로 생성되는 컨텐츠를 읽고 색인화할 수 있습니다.
  - 로봇 배제 표준(Robots Exclusion Protocol): 웹사이트 개발자들은 robots.txt 파일을 사용하여 크롤러가 접근할 수 있는 페이지를 지정할 수 있습니다. 이 파일은 특정 사용자 에이전트에 대한 크롤링 금지 규칙을 정의할 수 있습니다.
  - API 속도 제한 및 인증: 많은 API들이 속도 제한 또는 인증 메커니즘을 통해 무분별한 크롤링을 방지합니다. 이는 서버 과부하를 방지하고, 데이터의 무단 사용을 제한하는 데 도움이 됩니다.
  - 헤더 검사 및 세션 관리: 크롤러는 HTTP 헤더를 검사하여 캐시된 데이터를 사용하거나, 세션을 유지 관리함으로써 효율적으로 크롤링할 수 있습니다.
  - 사이트맵 사용: 웹사이트 개발자들은 사이트맵을 제공하여 크롤러가 효율적으로 사이트를 탐색하도록 할 수 있습니다. 이는 특히 대규모 사이트나 복잡한 동적 컨텐츠가 있는 사이트에서 유용합니다.
  - 인공 지능 및 머신 러닝 기술: 일부 고급 크롤러는 인공 지능과 머신 러닝 알고리즘을 사용하여 웹사이트의 구조를 이해하고, 효과적으로 동적 컨텐츠를 크롤링합니다.
### 루프와 중복 피하기
- 이걸 피하기 위해 휴리스틱의 집합을 필요로 하는데, 중복을 피하려 할수록 유효한 콘텐츠 거를 경우가 높아지는 트레이드 오프 존재
### 잘 동작시키도록 사용하는 기법
- url 정규화
- 너비 우선 크롤링
- 스로틀링
- url 크기 제한 : 요즘엔 url에 많은 정보를 담으므로 위험할 수 있다 [참고](https://www.youtube.com/watch?v=pCOBmmJARPE&t=172s)
- url/사이트 블랙리스트
- 패턴 발견 : 루프는 보통 일정한 패턴으로 url 에 뭔가 덧붙여지는데 이 패턴을 찾는거
- 콘텐츠 지문 : 체크섬을 통해(md5 같은) 중복 방지
- 사람의 모니터링 : 검수
## 9.2 로봇의 HTTP
- 로봇도 http 클라이언트 니까 명세를 지켜야함
- 요구사항이 적은 http/1.0 요청을 보내는 애들이 많은듯
### 요청 헤더 식별하기
- 신원 식별 헤더로 로봇의 능력,신원,출신 알려줌
- 기본적인 신원 식별 헤더 종류
  - User-Agent : 서버에게 요청을 만든 로봇의 이름 말해줌
  - From : 로봇의 사용자/관리자의 이메일주소 제공
  - Accept : 서버에게 어떤 미디어 타입 보내도 되는지 말해줌
  - Referer : 현재의 요청 url 을 포함한 문서의 url을 제공
### 가상 호스팅
- 로봇 구현자는 Host 헤더를 지원할 필요가 있다
- 없으면 가상 docroot 가 예상한게 아닌 다른 컨텐츠 줄수도 있다
### 조건부 요청
- 모디파이드 타임이나 엔티티 태그 를 통해 업데이트 된거만 알아보는 조건부 http 요청 구현
### 응답 다루기
- 대다수는 GET 으로 가져오기만 해서 응답다루기는 거의 안하지만, 다룰때도 있다
- 상태 코드
  - 200, 404 같은 거 이해해야함
- 엔티티
  - 메타 html 태그를 해석해야 할 때도 있음
### User-Agent 타겟팅
- 이해는 안되는데, 다양한 로봇에도 모두 컨텐츠를 제공할 수 있어야 한다는 의미인듯
## 부적절하게 동작하는 로봇들
- 폭주하는 로봇 : 아주 튼튼한 환경에서 돌아가는 로봇은 뭔가 문제가 생기면 웹서버에 폭격을 가한다
- 오래된 url
- 길고 잘못된 url
- 호기심이 지나친 로봇
- 동적 게이트웨이 접근
## 9.4 로봇 차단하기
- robots.txt : 로봇 접근 제어 정보 저장하는 파일
### 로봇 차단 표준
- 0.0 : 로봇 배제 표준-Disallow 지시자 지원하는 메커니즘
- 1.0 : 웹 로봇 제어 방법-Allow 지시자 지원 추가
- 2.0 : 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 확장. 널리 지원되지는 않음
- 오늘날 대부분은 0.0, 1.0임
### 웹 사이트와 robots.txt 파일들
- robots.txt 가져오기 실패하면 그냥 허용해주는줄 알고 접근함
### robots.txt 파일 포맷
- 빈줄, 주석 줄, 규칙 줄 세 종류
- 규칙줄은 http 헤더와 비슷(<필드>:<값>) ex. User-Agent: slurp
- User-Agent 줄 : 하나 이상 존재, 여러 로봇 허용가능한거
- Disallow와 Allow 줄들
  - 특정 로봇들에 대해 어떤 url 허용, 금지인지 기술, 먼저 매칭된 줄의 기술로 사용됨
- Disallow/Allow 접두 매칭 가능
- 빈 문자열은 모든 것에 매칭
- "/" 와 "%2F" 는 매칭 안됨
- robots.txt 는 다른 필드들도 포함될 수 있고, 근데 로봇은 자기가 모르는 필드는 무시해야한다
- 매 접근마다 robots.txt 가져오면 부하가 큼, 캐싱해야함
  - HTTP/1.1 아니면 캐시 지시자 이해 못하는 점에 주의하자
### HTML 로봇 제어 META 태그
- robots.txt 는 콘텐츠 작성자가 아닌 웹사이트 관리자가 소유한다는 단점 존재
- 그래서 HTML 문서에 직접 로봇 제어 태그를 추가 가능하다
- `<META NAME="ROBOTS" CONTENT=directive-list>`
### 로봇 META 지시자
- NOINDEX : 이페이지를 무시하라
- NOFOLLOW : 이 페이지가 링크한 페이지를 크롤링하지 마라
- INDEX : 이 페이지 콘텐츠 인덱싱 가능
- FOLLOW : 링크한 페이지 크롤링 가능
- NOARCHIVE : 이 페이지의 캐시를 위한 로컬 사본을 만들면 안된다
- ALL : INDEX, FOLLOW 합친거
- NONE : NOINDEX, NOFOLLOW 합친거
- 이 모든건 HTML HEAD 섹션에 나타나야 한다
### 검색엔진 META 태그
- 모든 로봇 META 태그는 name="robots"
- DESCRIPTION : 저자가 웹페이지 짧은 요약
- KEYWORDS : 키워드 검색을 돕기 위한 단어들, 쉼표로 구분
- REVISIT-AFTER : 이 페이지는 쉽게 변경될 것이기 때문에 지정된 날짜 만큼 지나면 다시 방문해야 한다고 지시
## 9.5 로봇 에티켓
### 신원 식별
- 로봇 신원 밝혀라
- 기계 신원 밝혀라
- 연락처 밝혀라
### 동작
- 긴장하라 : 항의 받을 준비해라
- 대비하라 : 자기네 네트워크 대역폭 엄청 잡아먹을거다
- 감시와 로그
- 배우고 조정하라 : 로봇 매번 조정하고 개선하여 함정 안빠지게 하라
### 스스로를 제한하라
- url을 필터링하라
- 동적 url을 필터링하라
- Accept 관련 헤더로 필터링
- robots.txt 따르라
- 스스로를 억제하라
### 루프와 중복을 견뎌내기, 그리고 그 외의 문제들
- 모든 응답 코드 다루기
- url 정규화하기
- 적극적으로 순환 피하기
- 함정을 감시
- 블랙리스트 관리
### 확장성
- 공간 이해하기 : 로봇이 들고오는 문제가 얼마나 큰지 미리 계산하라
- 대역폭 이해하기
- 시간 이해하기
- 분할 정복 : 대형 멀티 프로세서 서버 등 하드웨어를 많이 쓰자
### 신뢰성
- 철저하게 테스트하라
- 체크포인트 : 실패지점부터 다시 시작할 수 있게끔
- 실패에 대한 유연성
### 소통
- 준비하라
- 이해하라
- 즉각 대응하라
## 9.6 검색엔진
- 웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진
### 넓게 생각하라
- 페이지 검색을 위해 약 수십억개의 http 질의가 생성되는데, 건당 0.5초 걸린다 하면 5700일이 걸린다고 함 -> 이는 병렬로 많은 장비를 동원해야함
### 현대적인 검색엔진의 아키텍처
- 전세계 웹페이지들에 대해 `full-text indexes(풀 텍스트 색인)` 이라고 하는 복잡한 로컬 데이터베이스 생성 -> 색인은 일종의 카드 카탈로그처럼 동작
- 사용자들은 웹 검색 게이트웨이(네이버, 구글 등) 를 통해 풀텍스트 색인에 대한 질의를 보냄
- 크롤링에 걸리는 엄청난 시간에 비해, 웹사이트는 빠르게 변하므로 풀텍스트 색인은 웹의 특정 순간에 대한 스냅샷 인것임
### 풀 텍스트 색인
- 단어 하나를 입력받아, 그 단어를 포함하는 모든 문서를 즉각 알려줄 수 있는 데이터베이스
- 단어 단위로 쪼개 그 단어가 들어가는 문서를 기억(역색인 아닌가)
### 질의 보내기
- 사용자가 HTML 폼을 채워서, GET 혹은 POST 로 요청을 통해 게이트웨이로 보냄
### 검색 결과를 정렬하고 보여주기
- 질의 결과를 이용해 게이트웨이에서 결과페이지로 제작
- 결과가 여러개일 때 관련도 랭킹을 매겨야함
- 랭킹을 위해 크롤링과정에서 수집된 데이터를 사용
### 스푸핑
- 검색순위 상위권을 위해 웹개발자들이 전략을 짠다
- 검색엔진은 이런 스푸핑한것들을 거르기 위해 애쓴다
  

